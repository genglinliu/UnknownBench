model_name,refusal_rate_answerable,refusal_rate_unanswerable,delta,accuracy_answerable,num_answerable,num_answerable_RefuNQ,num_unanswerable
gpt-3.5-turbo-0613,8.7,51.7,43.0,48.3,6703.0,2266.0,6615.0
gpt-4-0613,12.1,65.1,53.0,53.7,6703.0,2266.0,6616.0
claude-2,8.1,43.1,35.1,37.3,6703.0,2266.0,6616.0
chat-bison-001,11.1,27.5,16.4,38.7,6677.0,2253.0,6594.0
Llama-2-7b-chat-hf,10.1,25.5,15.4,32.8,6703.0,2266.0,6615.0
Llama-2-7b-hf,7.1,7.4,0.3,32.6,6703.0,2266.0,6616.0
Llama-2-13b-chat-hf,13.4,30.9,17.5,37.9,6677.0,2253.0,6594.0
Llama-2-13b-hf,6.7,7.1,0.4,32.9,6703.0,2266.0,6616.0
Llama-2-70b-chat-hf,9.4,25.1,15.7,45.6,6703.0,2266.0,6616.0
vicuna-7b-v1.5,10.5,27.5,16.9,24.1,6703.0,2266.0,6616.0
vicuna-13b-v1.5,10.8,32.9,22.0,34.8,6703.0,2266.0,6616.0
